{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff1d74c",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Requerimientos Generales (aplican a todos los módulos)\n",
    "\n",
    "- **RG-1 (Lenguaje):** Todos los programas deben implementarse en **Python ≥ 3.8**.  \n",
    "- **RG-2 (Librerías):** Se deberá utilizar **NumPy** para operaciones numéricas y manejo de arreglos; **Pandas** para carga y escritura de datos `.csv`\n",
    "- **RG-3 (Claridad):** Cada archivo `.py` deberá incluir:  \n",
    "  - Encabezado con descripción del propósito.  \n",
    "  - Comentarios claros para cada función.  \n",
    "  - Nombres de variables autoexplicativos.  \n",
    "- **RG-4 (Reproducibilidad):** El código debe permitir replicar los resultados sin modificaciones adicionales al entorno o parámetros.  \n",
    "- **RG-5 (Gestión de errores):** Validar existencia y consistencia de los archivos `.csv` antes de procesarlos.  \n",
    "\n",
    "---\n",
    "\n",
    "## 2. Requerimientos de `ppr.py` (Preprocesamiento de Datos)\n",
    "\n",
    "- **RPPR-1:** Implementar función `get_features()` para calcular entropía:  \n",
    "  - Modo Dispersión → parámetros: `d`, `tau`, `c`.  \n",
    "  - Modo Permutación → parámetros: `d`, `tau`.  \n",
    "- **RPPR-2:** Leer los archivos de entrada: `class1.csv` y `class2.csv`.  \n",
    "- **RPPR-3:** Segmentar los datos en ventanas de tamaño `W` (filas).  \n",
    "- **RPPR-4:** Generar archivos de salida con características:  \n",
    "  - `dfeatures1.csv` → características extraídas de `class1.csv`.  \n",
    "  - `dfeatures2.csv` → características extraídas de `class2.csv`.  \n",
    "- **RPPR-5:** Concatenar ambos conjuntos de características en un solo archivo:  \n",
    "  - `dfeatures.csv` → de dimensión `(2K, W)`.  \n",
    "- **RPPR-6:** Crear archivo de etiquetas `label.csv`:  \n",
    "  - Clase 1 (`K/2` filas) → etiqueta `1`.  \n",
    "  - Clase 2 (`K/2` filas) → etiqueta `0`.  \n",
    "- **RPPR-7:** Incluir función `main()` que ejecute de forma secuencial:  \n",
    "  1. Lectura de datos.  \n",
    "  2. Generación de características.  \n",
    "  3. Concatenación y guardado.  \n",
    "  4. Creación de etiquetas.  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Requerimientos de `trn.py` (Entrenamiento y Evaluación)\n",
    "\n",
    "- **RTRN-1:** Cargar datos desde `dfeatures.csv` y `label.csv`.  \n",
    "- **RTRN-2:** Implementar un modelo de **Regresión Logística** entrenado mediante **mini-batch Gradient Descent (mGD)**.  \n",
    "- **RTRN-3:** Permitir ajuste de hiperparámetros:  \n",
    "  - Tasa de aprendizaje (`α`).  \n",
    "  - Tamaño de batch (`m`).  \n",
    "  - Número máximo de iteraciones.  \n",
    "- **RTRN-4:** Entrenar el modelo utilizando el conjunto de entrenamiento y guardar parámetros finales (pesos y bias).  \n",
    "- **RTRN-5:** Evaluar el modelo con conjunto de prueba:  \n",
    "  - Calcular matriz de confusión.  \n",
    "  - Calcular F1-score para cada clase binaria.  \n",
    "- **RTRN-6:** Generar reporte de rendimiento en pantalla o archivo (`results.txt`).  \n",
    "- **RTRN-7:** Incluir función `main()` que ejecute de forma secuencial:  \n",
    "  1. Carga de datos.  \n",
    "  2. Entrenamiento del modelo.  \n",
    "  3. Evaluación del modelo.  \n",
    "  4. Presentación de resultados.  \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Requerimientos de `utils.py` (Opcional, Librería de Apoyo)\n",
    "\n",
    "> Este módulo es opcional, pero recomendable para mantener el código modular y reutilizable.  \n",
    "\n",
    "- **RUTIL-1:** Implementar funciones auxiliares como:  \n",
    "  - `save_data(data, filename)` → guardar `.csv`.  \n",
    "  - `load_data(filename)` → cargar `.csv`.  \n",
    "  - `plot_confusion_matrix(cm)` → graficar matriz de confusión.  \n",
    "- **RUTIL-2:** Centralizar en este archivo todas las funciones comunes a `ppr.py` y `trn.py`.  \n",
    "- **RUTIL-3:** Garantizar que el código principal (`ppr.py` y `trn.py`) se mantenga limpio y enfocado en lógica principal.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4d8c53",
   "metadata": {},
   "source": [
    "## ppr.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718485da",
   "metadata": {},
   "source": [
    "Propósito  \n",
    "El propósito de este programa es crear nuevas características desde la data original utilizando la entropía de Dispersión y la entropía de Permutación.  \n",
    "\n",
    "Función Principal  \n",
    "Nombre de la función: `get_features()`  \n",
    "Descripción: Esta función crea características de entropía a partir de los archivos `class1.csv` y `class2.csv`.  \n",
    "\n",
    "Opciones de Cálculo de Entropía  \n",
    "1. Opción Dispersión  \n",
    "   - Parámetros:  \n",
    "     - d: dimensión embedding  \n",
    "     - tau: tiempo de retardo  \n",
    "     - c: número de clases  \n",
    "\n",
    "2. Opción Permutación  \n",
    "   - Parámetros:  \n",
    "     - d: dimensión embedding  \n",
    "     - tau: tiempo de retardo  \n",
    "\n",
    "Segmentación de Archivos  \n",
    "- Los archivos de clase original deben segmentarse en ventanas de tamaño `W` (número de filas).  \n",
    "\n",
    "Archivos de Salida  \n",
    "1. `dfeatures1.csv`  \n",
    "   - Derivado de `class1.csv`.  \n",
    "   - W columnas: cada columna corresponde a una característica de entropía.  \n",
    "   - K filas: cada fila representa una nueva muestra de características.  \n",
    "\n",
    "2. `dfeatures2.csv`  \n",
    "   - Derivado de `class2.csv`.  \n",
    "   - W columnas: cada columna corresponde a una característica de entropía.  \n",
    "   - K filas: cada fila representa una nueva muestra de características.  \n",
    "\n",
    "3. `dfeatures.csv`  \n",
    "   - Creado a partir de la concatenación de `dfeatures1.csv` y `dfeatures2.csv`.  \n",
    "   - Tamaño resultante:  \n",
    "     - 2K filas.  \n",
    "     - W columnas.  \n",
    "\n",
    "4. `label.csv`  \n",
    "   - Archivo de etiquetas asociado a las clases.  \n",
    "   - Clase 1: Y(1 : K/2) = 1  \n",
    "   - Clase 2: Y(K/2 + 1 : N) = 0  \n",
    "\n",
    "Estructura del Programa Principal (`main`)  \n",
    "```python\n",
    "def main():\n",
    "    conf_entropy()\n",
    "    load_data()\n",
    "    F1 = get_features(\"class1.csv\")\n",
    "    F2 = get_features(\"class2.csv\")\n",
    "    F = np.concatenate((F1, F2), axis=0)\n",
    "    save_data(F, \"dfeatures.csv\")\n",
    "    save_labels(\"label.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80ca68d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------\n",
    "# Create Features by use \n",
    "# Dispersion Entropy and Permutation entropy\n",
    "#----------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from utility import entropy_dispersion, entropy_permuta\n",
    "\n",
    "#----------------------------------------------\n",
    "# Load parameters Entropy\n",
    "#----------------------------------------------\n",
    "def conf_entropy():\n",
    "    conf = pd.read_csv(\"config/conf_ppr.csv\")\n",
    "    opt  = conf.loc[0, \"opt\"]    # \"dispersion\" o \"permutation\"\n",
    "    d    = int(conf.loc[0, \"d\"])\n",
    "    tau  = int(conf.loc[0, \"tau\"])\n",
    "    c    = int(conf.loc[0, \"c\"])\n",
    "    W    = int(conf.loc[0, \"W\"])  # tamaño de ventana\n",
    "    \n",
    "    return (opt, d, tau, c, W)\n",
    "\n",
    "#----------------------------------------------\n",
    "# Load Data\n",
    "#----------------------------------------------\n",
    "def load_data(nFile):\n",
    "    data = pd.read_csv(nFile, header=None).values\n",
    "    return data\n",
    "\n",
    "#----------------------------------------------\n",
    "# Obtain entropy : Dispersión or Permutación\n",
    "#----------------------------------------------\n",
    "def gets_entropy(x, opt, d, tau, c):\n",
    "    if opt == \"dispersion\":\n",
    "        entr = entropy_dispersion(x, d, tau, c)\n",
    "    elif opt == \"permutation\":\n",
    "        entr = entropy_permuta(x, d, tau)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown entropy option\")\n",
    "    return entr\n",
    "\n",
    "#----------------------------------------------\n",
    "# Obtain Features by use Entropy    \n",
    "#----------------------------------------------\n",
    "def gets_features(file, opt, d, tau, c, W):\n",
    "    X = load_data(file)\n",
    "    N, L = X.shape\n",
    "    K = N // W  # número de segmentos\n",
    "    \n",
    "    F = []\n",
    "    for k in range(K):\n",
    "        block = X[k*W:(k+1)*W, :]\n",
    "        feats = [gets_entropy(block[:, j], opt, d, tau, c) for j in range(L)]\n",
    "        F.append(feats)\n",
    "    F = np.array(F)\n",
    "    return F    \n",
    "\n",
    "#----------------------------------------------\n",
    "# Save Features\n",
    "#----------------------------------------------\n",
    "def save_data(F, fname=\"dfeatures.csv\"):\n",
    "    pd.DataFrame(F).to_csv(fname, header=False, index=False)\n",
    "\n",
    "def save_labels(K, fname=\"label.csv\"):\n",
    "    Y = np.concatenate((np.ones(K//2), np.zeros(K//2)))\n",
    "    pd.DataFrame(Y).to_csv(fname, header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06e1a5e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'opt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'opt'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mdef main():\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    opt, d, tau, c, W = conf_entropy()\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m      8\u001b[39m \u001b[33;03m    save_labels(F.shape[0], \"label.csv\")\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m opt, d, tau, c, W = \u001b[43mconf_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m F1 = gets_features(\u001b[33m\"\u001b[39m\u001b[33mdata/class1.csv\u001b[39m\u001b[33m\"\u001b[39m, opt, d, tau, c, W)\n\u001b[32m     14\u001b[39m F2 = gets_features(\u001b[33m\"\u001b[39m\u001b[33mdata/class2.csv\u001b[39m\u001b[33m\"\u001b[39m, opt, d, tau, c, W)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mconf_entropy\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconf_entropy\u001b[39m():\n\u001b[32m     14\u001b[39m     conf = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33mconfig/conf_ppr.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     opt  = \u001b[43mconf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m    \u001b[38;5;66;03m# \"dispersion\" o \"permutation\"\u001b[39;00m\n\u001b[32m     16\u001b[39m     d    = \u001b[38;5;28mint\u001b[39m(conf.loc[\u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33md\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     17\u001b[39m     tau  = \u001b[38;5;28mint\u001b[39m(conf.loc[\u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtau\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/pandas/core/indexing.py:1183\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1181\u001b[39m     key = \u001b[38;5;28mtuple\u001b[39m(com.apply_if_callable(x, \u001b[38;5;28mself\u001b[39m.obj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[32m   1182\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_scalar_access(key):\n\u001b[32m-> \u001b[39m\u001b[32m1183\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_tuple(key)\n\u001b[32m   1185\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1186\u001b[39m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/pandas/core/frame.py:4219\u001b[39m, in \u001b[36mDataFrame._get_value\u001b[39m\u001b[34m(self, index, col, takeable)\u001b[39m\n\u001b[32m   4216\u001b[39m     series = \u001b[38;5;28mself\u001b[39m._ixs(col, axis=\u001b[32m1\u001b[39m)\n\u001b[32m   4217\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m series._values[index]\n\u001b[32m-> \u001b[39m\u001b[32m4219\u001b[39m series = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_item_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4220\u001b[39m engine = \u001b[38;5;28mself\u001b[39m.index._engine\n\u001b[32m   4222\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.index, MultiIndex):\n\u001b[32m   4223\u001b[39m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[32m   4224\u001b[39m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[32m   4225\u001b[39m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/pandas/core/frame.py:4643\u001b[39m, in \u001b[36mDataFrame._get_item_cache\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m   4638\u001b[39m res = cache.get(item)\n\u001b[32m   4639\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4640\u001b[39m     \u001b[38;5;66;03m# All places that call _get_item_cache have unique columns,\u001b[39;00m\n\u001b[32m   4641\u001b[39m     \u001b[38;5;66;03m#  pending resolution of GH#33047\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4643\u001b[39m     loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4644\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._ixs(loc, axis=\u001b[32m1\u001b[39m)\n\u001b[32m   4646\u001b[39m     cache[item] = res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.13/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'opt'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "def main():\n",
    "    opt, d, tau, c, W = conf_entropy()\n",
    "    F1 = gets_features(\"data/class1.csv\", opt, d, tau, c, W)\n",
    "    F2 = gets_features(\"data/class2.csv\", opt, d, tau, c, W)\n",
    "    F  = np.concatenate((F1, F2), axis=0)\n",
    "    save_data(F, \"dfeatures.csv\")\n",
    "    save_labels(F.shape[0], \"label.csv\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "opt, d, tau, c, W = conf_entropy()\n",
    "F1 = gets_features(\"data/class1.csv\", opt, d, tau, c, W)\n",
    "F2 = gets_features(\"data/class2.csv\", opt, d, tau, c, W)\n",
    "F  = np.concatenate((F1, F2), axis=0)\n",
    "save_data(F, \"dfeatures.csv\")\n",
    "save_labels(F.shape[0], \"label.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231f286e",
   "metadata": {},
   "source": [
    "## trn.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71514c44",
   "metadata": {},
   "source": [
    "Etapa 2 – Algoritmo de Entrenamiento (trn.py)  \n",
    "\n",
    "Propósito  \n",
    "Este programa implementa el algoritmo de entrenamiento de un modelo de regresión logística, utilizando descenso de gradiente con momentum. Se encarga de dividir los datos en conjuntos de entrenamiento y prueba, entrenar el modelo y almacenar los resultados obtenidos.  \n",
    "\n",
    "Carga y Preparación de Datos  \n",
    "- Cargar archivos `dfeatures.csv` y `label.csv`.  \n",
    "- Reordenar aleatoriamente la matriz de características y el vector de clases para evitar sesgos.  \n",
    "- Crear función para dividir los datos en conjuntos de entrenamiento y prueba:  \n",
    "  - Datos de Entrenamiento:  \n",
    "    - Archivos de salida: `dtrn.csv` y `dtrn_label.csv`.  \n",
    "    - Número de muestras: L = round(N * p / 100).  \n",
    "    - p: porcentaje de entrenamiento (ej. 60 o 80).  \n",
    "  - Datos de Prueba:  \n",
    "    - Archivos de salida: `dtst.csv` y `dtst_label.csv`.  \n",
    "    - Número de muestras: K = N – L.  \n",
    "\n",
    "Función de Entrenamiento – trn_logistic()  \n",
    "- Cargar parámetros de configuración desde archivo `conf_train.csv`.  \n",
    "- Implementar algoritmo de **descenso de gradiente con momentum**.  \n",
    "- Aprender y actualizar los pesos del modelo en cada iteración.  \n",
    "- Guardar los resultados del entrenamiento en archivos:  \n",
    "  - `pesos.csv` → matriz de pesos finales del modelo.  \n",
    "  - `costo.csv` → vector de costos por iteración.  \n",
    "    - Número de filas: máximo de iteraciones.  \n",
    "    - Número de columnas: 1.  \n",
    "\n",
    "Estructura del Programa Principal (main)  \n",
    "```python\n",
    "def main():\n",
    "    conf_train()\n",
    "    load_data()\n",
    "    train()\n",
    "    save_w_cost(W, Cost, 'pesos.csv', 'costo.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc447a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3221381587.py, line 19)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m....\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression's Training :\n",
    "\n",
    "import numpy      as np\n",
    "import utility    as ut\n",
    "\n",
    "#Save weights and Cost\n",
    "def save_w_cost():\n",
    "    ...\n",
    "    return\n",
    "#\n",
    "def iniWs():\n",
    "    ...\n",
    "    return(W,V)\n",
    "# Load config train for Regression\n",
    "#\n",
    "#Training by use mGD\n",
    "def train():    \n",
    "    \n",
    "    ....\n",
    "    return()\n",
    "# Load data to train \n",
    "def load_data():\n",
    "    \n",
    "    \n",
    "    return()\n",
    "#\n",
    "def conf_train(nFile):\n",
    "    ....\n",
    "    return()\n",
    "\n",
    "# Beginning ...\n",
    "def main():    \n",
    "    conf_train()\n",
    "    load_data()   \n",
    "    train()             \n",
    "    save_w_cost(W,Cost, 'pesos.csv','costo.csv')\n",
    "       \n",
    "if __name__ == '__main__':   \n",
    "\t main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0fd2fe",
   "metadata": {},
   "source": [
    "## tst.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f512b514",
   "metadata": {},
   "source": [
    "Etapa 3 – Evaluación del Modelo (eval.py)  \n",
    "\n",
    "Cargar data de test:  \n",
    "- Cargar los coeficientes desde archivo `pesos.csv`.  \n",
    "- Generar los valores estimados usando Regresión Logit.  \n",
    "\n",
    "Crear archivos de resultados:  \n",
    "- Matriz de Confusión → `cmatriz.csv`, matriz de dimensión (2,2).  \n",
    "- F-scores → `fscores.csv`, vector de dimensión (1,2).  \n",
    "\n",
    "Programa Principal:  \n",
    "```python\n",
    "def main():\n",
    "    load_data()\n",
    "    load_w()\n",
    "    zv = forward(xv, W)\n",
    "    cm, Fsc = metricas(yv, zv)\n",
    "    save_measure(cm, Fsc, 'cmatrix.csv', 'fscores.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ff6407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for Logistic Regresion\n",
    "import numpy as np\n",
    "\n",
    "def forward(xv,w):\n",
    "    ...\n",
    "    return(zv)\n",
    "#\n",
    "def measure(yv,zv):\n",
    "    ...\n",
    "    return(cmatrix,Fscores)\n",
    "#\n",
    "def save_measure(cm,Fsc,nFile1,nFile2):\n",
    "    ...\n",
    "    return()\n",
    "# Load weight\n",
    "def load_w(nFile):\n",
    "    ...\n",
    "    return(W)\n",
    "# \n",
    "def load_data(nFile):\n",
    "    ....\n",
    "    return(x,y)\n",
    "# Beginning ...\n",
    "def main():\t\t\t\n",
    "\tload_data()\n",
    "\tload_w()\n",
    "\tzv     = forward(xv,W)      \t\t\n",
    "\tcm,Fsc = metricas(yv,zv) \t\n",
    "\tsave_measure(cm,Fsc,'cmatrix.csv','Fscores.csv')\t\t\n",
    "\n",
    "if __name__ == '__main__':   \n",
    "\t main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4f7b7e",
   "metadata": {},
   "source": [
    "## utility.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e099fda7",
   "metadata": {},
   "source": [
    "## Entropía de Dispersión (DE) (PTT 6)\n",
    "\n",
    "La entropía de dispersión es un método para cuantificar la incertidumbre de una serie temporal, utilizando patrones embebidos y simbolización discreta.\n",
    "\n",
    "**Normalización**  \n",
    "El vector de datos $X$ se normaliza entre 0 y 1:\n",
    "\n",
    "$$\n",
    "x_i^{norm} = \\frac{x_i - \\min(X)}{\\max(X) - \\min(X)}\n",
    "$$\n",
    "\n",
    "**Embedding**  \n",
    "Se construyen vectores embebidos de dimensión $d$ y retardo $\\tau$:\n",
    "\n",
    "$$\n",
    "X_i = \\{x_i, x_{i+\\tau}, \\ldots, x_{i+(d-1)\\tau}\\}\n",
    "$$\n",
    "\n",
    "Número total de vectores:\n",
    "\n",
    "$$\n",
    "M = N - (d-1)\\tau\n",
    "$$\n",
    "\n",
    "**Simbolización**  \n",
    "Cada vector $X_i$ se transforma en símbolos discretos:\n",
    "\n",
    "$$\n",
    "Y_i = \\mathrm{round}(0.5 + X_i \\cdot c)\n",
    "$$\n",
    "\n",
    "**Probabilidades**  \n",
    "La probabilidad de ocurrencia de cada patrón es:\n",
    "\n",
    "$$\n",
    "p_k = \\frac{f_k}{M}, \\quad k = 1,2,\\ldots,c^d\n",
    "$$\n",
    "\n",
    "**Entropía de Dispersión**  \n",
    "La entropía se define como:\n",
    "\n",
    "$$\n",
    "DE = -\\sum_{k=1}^{c^d} p_k \\log(p_k)\n",
    "$$\n",
    "\n",
    "**Entropía Normalizada**  \n",
    "\n",
    "$$\n",
    "DE_{norm} = \\frac{DE}{\\log(c^d)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c4f949",
   "metadata": {},
   "source": [
    "#### Entropía de Permutación (PE) (PTT 4)\n",
    "\n",
    "La entropía de permutación es un método para cuantificar la complejidad de una serie temporal a partir del orden relativo de sus valores, utilizando patrones ordinales.\n",
    "\n",
    "**Embedding**  \n",
    "Se construyen vectores embebidos de dimensión $d$ y retardo $\\tau$:\n",
    "\n",
    "$$\n",
    "X_i = (x_i, x_{i+\\tau}, \\ldots, x_{i+(d-1)\\tau})\n",
    "$$\n",
    "\n",
    "Número total de vectores:\n",
    "\n",
    "$$\n",
    "M = N - (d-1)\\tau\n",
    "$$\n",
    "\n",
    "**Número de patrones posibles**  \n",
    "\n",
    "$$\n",
    "d!\n",
    "$$\n",
    "\n",
    "**Probabilidades**  \n",
    "La probabilidad de ocurrencia de cada patrón es:\n",
    "\n",
    "$$\n",
    "p_k = \\frac{f_k}{M}, \\quad k = 1,2,\\ldots,d!\n",
    "$$\n",
    "\n",
    "**Entropía de Permutación**  \n",
    "La entropía se define como:\n",
    "\n",
    "$$\n",
    "PE = -\\sum_{r=1}^{d!} p_r \\log(p_r)\n",
    "$$\n",
    "\n",
    "**Entropía Normalizada**  \n",
    "\n",
    "$$\n",
    "nPE = \\frac{PE}{\\log(d!)}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7accf246",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# My Utility : auxiliars functions\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# CResidual-Dispersion Entropy\n",
    "# ---------------------------------------------------------\n",
    "def entropy_dispersion(x, d, tau, c):\n",
    "    \"\"\"\n",
    "    Calcula la Entropía de Dispersión (DE).\n",
    "    Parámetros:\n",
    "    x   : array-like, serie temporal\n",
    "    d   : dimensión de embedding\n",
    "    tau : retardo\n",
    "    c   : número de símbolos\n",
    "    \"\"\"\n",
    "    x = np.array(x, dtype=float)\n",
    "    N = len(x)\n",
    "\n",
    "    # Paso 1: Normalizar entre [0,1]\n",
    "    x_norm = (x - np.min(x)) / (np.max(x) - np.min(x) + 1e-12)\n",
    "\n",
    "    # Paso 2: Embedding\n",
    "    M = N - (d - 1) * tau\n",
    "    if M <= 0:\n",
    "        raise ValueError(\"Serie demasiado corta para embedding\")\n",
    "    X_emb = np.array([x_norm[i:i + d * tau:tau] for i in range(M)])\n",
    "\n",
    "    # Paso 3: Simbolización\n",
    "    Y = np.round(0.5 + X_emb * c).astype(int)\n",
    "    Y[Y < 1] = 1\n",
    "    Y[Y > c] = c\n",
    "\n",
    "    # Paso 4: Patrones (convertir cada vector en número base-c)\n",
    "    patrones = []\n",
    "    for y in Y:\n",
    "        code = 0\n",
    "        for j in range(d):\n",
    "            code += (y[j] - 1) * (c ** j)\n",
    "        patrones.append(code)\n",
    "    patrones = np.array(patrones)\n",
    "\n",
    "    # Paso 5: Frecuencias\n",
    "    unique, counts = np.unique(patrones, return_counts=True)\n",
    "    probs = counts / M\n",
    "\n",
    "    # Paso 6: Entropía de Shannon\n",
    "    entr = -np.sum(probs * np.log(probs + 1e-12))\n",
    "\n",
    "    # Paso 7: Normalización\n",
    "    entr = entr / np.log(c ** d)\n",
    "\n",
    "    return entr\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Permutation Entropy\n",
    "# ---------------------------------------------------------\n",
    "def entropy_permuta(x, m, tau):\n",
    "    \"\"\"\n",
    "    Calcula la Entropía de Permutación (PE).\n",
    "    Parámetros:\n",
    "    x   : array-like, serie temporal\n",
    "    m   : dimensión de embedding\n",
    "    tau : retardo\n",
    "    \"\"\"\n",
    "    x = np.array(x, dtype=float)\n",
    "    N = len(x)\n",
    "\n",
    "    # Paso 1: Embedding\n",
    "    M = N - (m - 1) * tau\n",
    "    if M <= 0:\n",
    "        raise ValueError(\"Serie demasiado corta para embedding\")\n",
    "    X_emb = np.array([x[i:i + m * tau:tau] for i in range(M)])\n",
    "\n",
    "    # Paso 2: Patrones ordinales\n",
    "    patrones = []\n",
    "    for row in X_emb:\n",
    "        perm = tuple(np.argsort(row))  # orden relativo\n",
    "        patrones.append(perm)\n",
    "    patrones = np.array(patrones)\n",
    "\n",
    "    # Paso 3: Frecuencias\n",
    "    unique, counts = np.unique(patrones, axis=0, return_counts=True)\n",
    "    probs = counts / M\n",
    "\n",
    "    # Paso 4: Entropía de Shannon\n",
    "    entr = -np.sum(probs * np.log(probs + 1e-12))\n",
    "\n",
    "    # Paso 5: Normalización\n",
    "    from math import factorial, log\n",
    "    entr = entr / log(factorial(m))\n",
    "\n",
    "    return entr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f64b9639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 1. Serie constante ===\n",
      "DE (const): -3.03440049141233e-13\n",
      "PE (const): -5.581602429106793e-13\n",
      "\n",
      "=== 2. Serie aleatoria uniforme ===\n",
      "DE (random): 0.9574806450803466\n",
      "PE (random): 0.9958247165318197\n",
      "\n",
      "=== 3. Serie senoidal ===\n",
      "DE (sinusoidal): 0.5031380541975659\n",
      "PE (sinusoidal): 0.4967356215755387\n",
      "\n",
      "=== 4. Sensibilidad a parámetros ===\n",
      "DE (d=2): 0.8022\n",
      "DE (d=3): 0.7773\n",
      "DE (d=4): 0.7286\n",
      "PE (m=2): 1.0000\n",
      "PE (m=3): 0.9972\n",
      "PE (m=4): 0.9872\n",
      "\n",
      "=== 5. Comparación en señal mixta (sin+ruido) ===\n",
      "DE (mixta): 0.6435679277903418\n",
      "PE (mixta): 0.996928687422783\n"
     ]
    }
   ],
   "source": [
    "def test_entropies():\n",
    "    np.random.seed(0)\n",
    "\n",
    "    print(\"\\n=== 1. Serie constante ===\")\n",
    "    x_const = np.ones(100)\n",
    "    print(\"DE (const):\", entropy_dispersion(x_const, d=3, tau=1, c=3))\n",
    "    print(\"PE (const):\", entropy_permuta(x_const, m=3, tau=1))\n",
    "\n",
    "    print(\"\\n=== 2. Serie aleatoria uniforme ===\")\n",
    "    x_rand = np.random.rand(500)\n",
    "    print(\"DE (random):\", entropy_dispersion(x_rand, d=3, tau=1, c=6))\n",
    "    print(\"PE (random):\", entropy_permuta(x_rand, m=3, tau=1))\n",
    "\n",
    "    print(\"\\n=== 3. Serie senoidal ===\")\n",
    "    t = np.linspace(0, 10*np.pi, 500)\n",
    "    x_sin = np.sin(t)\n",
    "    print(\"DE (sinusoidal):\", entropy_dispersion(x_sin, d=3, tau=2, c=6))\n",
    "    print(\"PE (sinusoidal):\", entropy_permuta(x_sin, m=3, tau=2))\n",
    "\n",
    "    print(\"\\n=== 4. Sensibilidad a parámetros ===\")\n",
    "    x_test = np.random.randn(300)\n",
    "    for d in [2, 3, 4]:\n",
    "        print(f\"DE (d={d}): {entropy_dispersion(x_test, d=d, tau=1, c=5):.4f}\")\n",
    "    for m in [2, 3, 4]:\n",
    "        print(f\"PE (m={m}): {entropy_permuta(x_test, m=m, tau=1):.4f}\")\n",
    "\n",
    "    print(\"\\n=== 5. Comparación en señal mixta (sin+ruido) ===\")\n",
    "    x_cmp = np.sin(np.linspace(0, 4*np.pi, 200)) + 0.2*np.random.randn(200)\n",
    "    print(\"DE (mixta):\", entropy_dispersion(x_cmp, d=3, tau=1, c=6))\n",
    "    print(\"PE (mixta):\", entropy_permuta(x_cmp, m=3, tau=1))\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Ejecutar testeo\n",
    "# -------------------------------------------------\n",
    "test_entropies()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
